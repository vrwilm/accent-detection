{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81a713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46789135",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data/filtered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d61356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'german', 'mandarin', 'russian', 'english']\n"
     ]
    }
   ],
   "source": [
    "# # note .DS_store needs to be removed\n",
    "# languages = os.listdir(\"./data/filtered\")\n",
    "# print(languages) \n",
    "# languages = languages[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cae9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'german': 0, 'mandarin': 1, 'russian': 2, 'english': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lang_mapping = {l: i for i, l in enumerate(languages)}\n",
    "# lang_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a720f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_files_by_lang = {}\n",
    "\n",
    "# for lang in languages:\n",
    "#     audio_files_by_lang[lang] = glob(f\"./data/filtered/{lang}/*.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea82bdd",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## trimming\n",
    "trimming at 20 db to remove leadingg and trailing silence. Here's what it means:<br>\n",
    "10 dB: Normal breathing<br>\n",
    "20 dB: Whispering from five feet away<br>\n",
    "30 dB: Whispering nearby<br>\n",
    "40 dB: Quiet library sounds\n",
    "\n",
    "to play the trimmed audio:<br>\n",
    "signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)<br>\n",
    "ipd.Audio(data=signal_trimmed, rate=sample_rate)<br>\n",
    "\n",
    "## amplitude_to_db\n",
    "https://stackoverflow.com/questions/63347977/what-is-the-conceptual-purpose-of-librosa-amplitude-to-db\n",
    "\n",
    "The range of perceivable sound pressure is very wide, from around 20 Î¼Pa (micro Pascal) to 20 Pa, a ratio of 1 million. Furthermore the human perception of sound levels is not linear, but better approximated by a logarithm.\n",
    "\n",
    "By converting to decibels (dB) the scale becomes logarithmic. This limits the numerical range, to something like 0-120 dB instead. The intensity of colors when this is plotted corresponds more closely to what we hear than if one used a linear scale.\n",
    "\n",
    "Note that the reference (0 dB) point in decibels can be chosen freely. The default for librosa.amplitude_to_db is to compute numpy.max, meaning that the max value of the input will be mapped to 0 dB. All other values will then be negative. The function also applies a threshold on the range of sounds, by default 80 dB. So anything lower than -80 dB will be clipped -80 dB.\n",
    "\n",
    "### another parameter to try in amplitude_to_db -> add ref=np.max (also in librosa library ex): \n",
    "from Rob Mulla (2022): \"Audio Data Processing in Python\" (YouTube Tutorial)<br>\n",
    "log_spectogram = librosa.amplitude_to_db(spectogram, ref=np.max)\n",
    "\n",
    "\n",
    "## STFT window size\n",
    "https://librosa.org/doc/latest/generated/librosa.stft.html#librosa.stft\n",
    "\n",
    "n_fft = length of the windowed signal after padding with zeros. The number of rows in the STFT matrix D is (1 + n_fft/2). **The default value, n_fft=2048 samples**, corresponds to a physical duration of 93 milliseconds at a sample rate of 22050 Hz, i.e. the default sample rate in librosa. This value is well adapted for **music signals**. However, in **speech processing, the recommended value is 512**, corresponding to 23 milliseconds at a sample rate of 22050 Hz. In any case, we recommend setting n_fft to a power of two for optimizing the speed of the fast Fourier transform (FFT) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832fb61",
   "metadata": {},
   "source": [
    "## Preprocessing decisions:\n",
    "\n",
    "1. Trim leading and trailing silence from an audio signal (below 20 db is considered as silence)\n",
    "2. Cut signal to specified duration - 16 sec\n",
    "3. STFT window size = 512 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6abf311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = audio_files[-10]\n",
    "# signal, sample_rate = librosa.load(file, mono=True)\n",
    "# print(signal.shape)\n",
    "# signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "# print(signal_trimmed.shape)\n",
    "# signal_cut = signal_trimmed[:SIGNAL_LEN]\n",
    "# print(signal_cut.shape)\n",
    "# signal_padded = librosa.util.fix_length(signal_trimmed, size=363262)\n",
    "# print(signal_padded.shape)\n",
    "# ipd.Audio(signal_padded, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f0529f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b945ce3f3348efb9690b4caed7297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_FFT = 512\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "# will trim data to this duration in sec\n",
    "DURATION = 16\n",
    "SIGNAL_LEN = SAMPLE_RATE * DURATION\n",
    "\n",
    "# dictionary to store mapping, labels, and spectograms\n",
    "data = {\n",
    "    \"mapping\": [],\n",
    "    \"labels\": [],\n",
    "    \"stft\": []\n",
    "}\n",
    "\n",
    "audio_files = glob(f\"./data/filtered/*/*.wav\")\n",
    "\n",
    "# instantiate the progress bar\n",
    "f = IntProgress(min=0, max=len(audio_files)) \n",
    "display(f) # display the bar\n",
    "    \n",
    "for file in audio_files:\n",
    "    f.value += 1\n",
    "    lang = file.split(\"/\")[3]\n",
    "    data[\"mapping\"].append(lang)\n",
    "    data[\"labels\"].append(lang_mapping[lang])  \n",
    "\n",
    "    signal, sample_rate = librosa.load(file, mono=True)\n",
    "    \n",
    "    # Trim leading and trailing silence from an audio signal\n",
    "    signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "    \n",
    "    # cut signal to specified duration or pad with 0s\n",
    "    audio_duration = librosa.get_duration(y=signal_trimmed)\n",
    "    if audio_duration > DURATION:\n",
    "        signal_fin = signal_trimmed[:SIGNAL_LEN]\n",
    "    else:\n",
    "        signal_fin = librosa.util.fix_length(signal_trimmed, size=SIGNAL_LEN)\n",
    "\n",
    "    # 1. STFT\n",
    "    stft = librosa.stft(signal_fin, n_fft=N_FFT)\n",
    "    spectogram = np.abs(stft)\n",
    "    log_spectogram = librosa.amplitude_to_db(spectogram)\n",
    "    #log_spectogram = librosa.amplitude_to_db(spectogram, ref=np.max)\n",
    "\n",
    "    #convert to list to serialize with json (json can't work with ndarray)\n",
    "    data[\"stft\"].append(log_spectogram.tolist())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6f982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"./data/stft_data_16sec.json\"\n",
    "\n",
    "if os.path.exists(JSON_PATH):\n",
    "    os.remove(JSON_PATH)\n",
    "\n",
    "with open(JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49870a",
   "metadata": {},
   "source": [
    "## MFCC v1: n_fft = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59110b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d836c565d74e36a6c20dd134123e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_FFT = 512\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "# will trim data to this duration in sec\n",
    "DURATION = 16\n",
    "SIGNAL_LEN = SAMPLE_RATE * DURATION\n",
    "NUM_MFCC = 13\n",
    "\n",
    "# dictionary to store mapping, labels, and spectograms\n",
    "data_mfcc = {\n",
    "    \"mapping\": [],\n",
    "    \"labels\": [],\n",
    "    \"mfcc\": []\n",
    "}\n",
    "\n",
    "audio_files = glob(f\"./data/filtered/*/*.wav\")\n",
    "\n",
    "# instantiate the progress bar\n",
    "f = IntProgress(min=0, max=len(audio_files)) \n",
    "display(f) # display the bar\n",
    "    \n",
    "for file in audio_files:\n",
    "    f.value += 1\n",
    "    lang = file.split(\"/\")[3]\n",
    "    data_mfcc[\"mapping\"].append(lang)\n",
    "    data_mfcc[\"labels\"].append(lang_mapping[lang])  \n",
    "\n",
    "    signal, sample_rate = librosa.load(file, mono=True)\n",
    "    \n",
    "    # Trim leading and trailing silence from an audio signal\n",
    "    signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "    \n",
    "    # cut signal to specified duration or pad with 0s\n",
    "    audio_duration = librosa.get_duration(y=signal_trimmed)\n",
    "    if audio_duration > DURATION:\n",
    "        signal_fin = signal_trimmed[:SIGNAL_LEN]\n",
    "    else:\n",
    "        signal_fin = librosa.util.fix_length(signal_trimmed, size=SIGNAL_LEN)\n",
    "\n",
    "    \n",
    "    # 2. MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal_fin, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT)\n",
    "    mfcc = mfcc.T\n",
    "    data_mfcc[\"mfcc\"].append(mfcc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5122c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_JSON_PATH = \"./data/mfcc_data_16sec.json\"\n",
    "\n",
    "if os.path.exists(MFCC_JSON_PATH):\n",
    "    os.remove(MFCC_JSON_PATH)\n",
    "\n",
    "with open(MFCC_JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data_mfcc, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6139bf",
   "metadata": {},
   "source": [
    "# MFCC v2: n_fft =255; hop_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9bd82",
   "metadata": {},
   "source": [
    "Tensorflow tutorial on keywords classification uses: <br>\n",
    "frame_length=255, frame_step=128 (aka n_fft and hop_length?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1dcdc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071d0bf73d9f40bca9ca92642351072c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_FFT = 255\n",
    "HOP_LENGTH = 128\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "# will trim data to this duration in sec\n",
    "DURATION = 16\n",
    "SIGNAL_LEN = SAMPLE_RATE * DURATION\n",
    "NUM_MFCC = 11\n",
    "\n",
    "# dictionary to store mapping, labels, and spectograms\n",
    "data_mfcc_v2 = {\n",
    "    \"mapping\": [],\n",
    "    \"labels\": [],\n",
    "    \"mfcc\": []\n",
    "}\n",
    "\n",
    "audio_files = glob(f\"./data/filtered/*/*.wav\")\n",
    "\n",
    "# instantiate the progress bar\n",
    "f = IntProgress(min=0, max=len(audio_files)) \n",
    "display(f) # display the bar\n",
    "    \n",
    "for file in audio_files:\n",
    "    f.value += 1\n",
    "    lang = file.split(\"/\")[3]\n",
    "    data_mfcc_v2[\"mapping\"].append(lang)\n",
    "    data_mfcc_v2[\"labels\"].append(lang_mapping[lang])  \n",
    "\n",
    "    signal, sample_rate = librosa.load(file, mono=True)\n",
    "    \n",
    "    # Trim leading and trailing silence from an audio signal\n",
    "    signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "    \n",
    "    # cut signal to specified duration or pad with 0s\n",
    "    audio_duration = librosa.get_duration(y=signal_trimmed)\n",
    "    if audio_duration > DURATION:\n",
    "        signal_fin = signal_trimmed[:SIGNAL_LEN]\n",
    "    else:\n",
    "        signal_fin = librosa.util.fix_length(signal_trimmed, size=SIGNAL_LEN)\n",
    "\n",
    "    \n",
    "    # 2. MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal_fin, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    mfcc = mfcc.T\n",
    "    data_mfcc_v2[\"mfcc\"].append(mfcc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ba7c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_JSON_PATH = \"./data/mfcc_data_16sec_nfft255_nmfcc11.json\"\n",
    "\n",
    "if os.path.exists(MFCC_JSON_PATH):\n",
    "    os.remove(MFCC_JSON_PATH)\n",
    "\n",
    "with open(MFCC_JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data_mfcc_v2, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac1a9e",
   "metadata": {},
   "source": [
    "# Learning from errors:\n",
    "had problem with uneven array size & np.VisibleDeprecationWarning -> trimming audios made 1 audio file less than 16 sec, which resulted into ragged array -> couldn't properly import as np.array <br>\n",
    "**Solution**: pad with 0s using librosa.util.fix_length<br>\n",
    "Problem desc: <br>\n",
    "\"I faced the np.VisibleDeprecationWarning while stacking lists containing audio data from WAV files. This problem occurred because audio files had different lengths. So, the lists that I needed to stack together into one numpy array also had varying lengths. Ignoring or suppressing this warning did not give the desired stacked np array so, I made all the audio files of the same length by using pydub.AudioSegment as mentioned in this answer.\"\n",
    "https://stackoverflow.com/questions/63097829/debugging-numpy-visibledeprecationwarning-ndarray-from-ragged-nested-sequences\n",
    "\n",
    "\n",
    "UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
    "https://stackoverflow.com/questions/56929874/what-is-the-warning-empty-filters-detected-in-mel-frequency-basis-about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156c942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

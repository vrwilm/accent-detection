{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f81a713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea82bdd",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## trimming\n",
    "trimming at 20 db to remove leadingg and trailing silence. Here's what it means:<br>\n",
    "10 dB: Normal breathing<br>\n",
    "20 dB: Whispering from five feet away<br>\n",
    "30 dB: Whispering nearby<br>\n",
    "40 dB: Quiet library sounds\n",
    "\n",
    "to play the trimmed audio:<br>\n",
    "signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)<br>\n",
    "ipd.Audio(data=signal_trimmed, rate=sample_rate)<br>\n",
    "\n",
    "## amplitude_to_db\n",
    "https://stackoverflow.com/questions/63347977/what-is-the-conceptual-purpose-of-librosa-amplitude-to-db\n",
    "\n",
    "The range of perceivable sound pressure is very wide, from around 20 Î¼Pa (micro Pascal) to 20 Pa, a ratio of 1 million. Furthermore the human perception of sound levels is not linear, but better approximated by a logarithm.\n",
    "\n",
    "By converting to decibels (dB) the scale becomes logarithmic. This limits the numerical range, to something like 0-120 dB instead. The intensity of colors when this is plotted corresponds more closely to what we hear than if one used a linear scale.\n",
    "\n",
    "Note that the reference (0 dB) point in decibels can be chosen freely. The default for librosa.amplitude_to_db is to compute numpy.max, meaning that the max value of the input will be mapped to 0 dB. All other values will then be negative. The function also applies a threshold on the range of sounds, by default 80 dB. So anything lower than -80 dB will be clipped -80 dB.\n",
    "\n",
    "### another parameter to try in amplitude_to_db -> add ref=np.max (also in librosa library ex): \n",
    "from Rob Mulla (2022): \"Audio Data Processing in Python\" (YouTube Tutorial)<br>\n",
    "log_spectogram = librosa.amplitude_to_db(spectogram, ref=np.max)\n",
    "\n",
    "\n",
    "## STFT window size\n",
    "https://librosa.org/doc/latest/generated/librosa.stft.html#librosa.stft\n",
    "\n",
    "n_fft = length of the windowed signal after padding with zeros. The number of rows in the STFT matrix D is (1 + n_fft/2). **The default value, n_fft=2048 samples**, corresponds to a physical duration of 93 milliseconds at a sample rate of 22050 Hz, i.e. the default sample rate in librosa. This value is well adapted for **music signals**. However, in **speech processing, the recommended value is 512**, corresponding to 23 milliseconds at a sample rate of 22050 Hz. In any case, we recommend setting n_fft to a power of two for optimizing the speed of the fast Fourier transform (FFT) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832fb61",
   "metadata": {},
   "source": [
    "## Preprocessing decisions:\n",
    "\n",
    "1. Trim leading and trailing silence from an audio signal (below 20 db is considered as silence)\n",
    "2. Cut signal to a specified duration or zero-pad if shorter\n",
    "3. STFT/MFCC window size = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c3f78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(method, process_info, dataset_path, duration, sample_rate=22050, n_fft=512, num_mfcc=13):\n",
    "    data = {\n",
    "        \"process_info\": process_info,\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"stft\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"cwt\":[]\n",
    "    }\n",
    "    \n",
    "    if '.DS_Store' in os.listdir(DATASET_PATH):\n",
    "        os.remove(f'{DATASET_PATH}/.DS_Store')\n",
    "\n",
    "    languages = os.listdir(DATASET_PATH)\n",
    "    lang_mapping = {l: i for i, l in enumerate(languages)}\n",
    "\n",
    "\n",
    "    audio_files = glob(f\"{dataset_path}/*/*.wav\")\n",
    "    # instantiate the progress bar\n",
    "    f = IntProgress(min=0, max=len(audio_files)) \n",
    "    display(f) # display the bar\n",
    "\n",
    "    for file in audio_files:\n",
    "        f.value += 1\n",
    "        lang_pos = len(dataset_path.split(\"/\"))\n",
    "        lang = file.split(\"/\")[lang_pos]\n",
    "\n",
    "        data[\"mapping\"].append(lang)\n",
    "        data[\"labels\"].append(lang_mapping[lang])  \n",
    "\n",
    "        signal, sample_rate = librosa.load(file, mono=True)\n",
    "\n",
    "        # Trim leading and trailing silence from an audio signal\n",
    "        signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "\n",
    "        # cut signal to specified duration or pad with 0s\n",
    "        audio_duration = librosa.get_duration(y=signal_trimmed)\n",
    "        signal_len = sample_rate * duration\n",
    "        if audio_duration > duration:\n",
    "            signal_fin = signal_trimmed[:signal_len]\n",
    "        else:\n",
    "            signal_fin = librosa.util.fix_length(signal_trimmed, size=signal_len)\n",
    "        \n",
    "        if method == \"stft\":\n",
    "            stft = librosa.stft(signal_fin, n_fft=n_fft)\n",
    "            spectogram = np.abs(stft)\n",
    "            log_spectogram = librosa.amplitude_to_db(spectogram)\n",
    "            #log_spectogram = librosa.amplitude_to_db(spectogram, ref=np.max)\n",
    "\n",
    "            #convert to list to serialize with json (json can't work with ndarray)\n",
    "            data[\"stft\"].append(log_spectogram.tolist())\n",
    "        \n",
    "        if method == \"mfcc\":\n",
    "            mfcc = librosa.feature.mfcc(y=signal_fin, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft)\n",
    "            mfcc = mfcc.T\n",
    "            data[\"mfcc\"].append(mfcc.tolist())\n",
    "            \n",
    "#         if method=\"cwt\":\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b69b17",
   "metadata": {},
   "source": [
    "# 1. STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4e49c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb3383c9b744adeb9a4b696c3ef8142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_PATH = \"./data/filtered\"\n",
    "METHOD = \"stft\"\n",
    "\n",
    "N_FFT = 512\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 23\n",
    "\n",
    "PROCESS_INFO = {\"method\": METHOD, \"duration\": DURATION, \"n_fft\": N_FFT, \"sample_rate\": SAMPLE_RATE, \"trim\": \"True\", \"zero-pad\": \"True\"}\n",
    "\n",
    "data = extract_features(METHOD, PROCESS_INFO, DATASET_PATH, DURATION, SAMPLE_RATE, n_fft=N_FFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c4f8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "JSON_PATH = f\"./data/{METHOD}_{date}.json\"\n",
    "\n",
    "with open(JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd2e77",
   "metadata": {},
   "source": [
    "# 2. MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8f39ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda5a95071c4a8581205a86b85d667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_PATH = \"./data/filtered\"\n",
    "METHOD = \"mfcc\"\n",
    "\n",
    "N_FFT = 512\n",
    "NUM_MFCC = 13\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 23\n",
    "\n",
    "PROCESS_INFO = {\"method\": METHOD, \"duration\": DURATION, \"n_fft\": N_FFT, \"sample_rate\": SAMPLE_RATE, \"trim\": \"True\", \"zero-pad\": \"True\"}\n",
    "\n",
    "data = extract_features(METHOD, PROCESS_INFO, DATASET_PATH, DURATION, SAMPLE_RATE, n_fft=N_FFT, num_mfcc=NUM_MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e40d5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "JSON_PATH = f\"./data/{METHOD}_{date}.json\"\n",
    "\n",
    "with open(JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada2b76",
   "metadata": {},
   "source": [
    "# 3. CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data/filtered\"\n",
    "METHOD = \"cwt\"\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 23\n",
    "\n",
    "PROCESS_INFO = {\"method\": METHOD, \"duration\": DURATION, \"n_fft\": N_FFT, \"sample_rate\": SAMPLE_RATE, \"trim\": \"True\", \"zero-pad\": \"True\"}\n",
    "\n",
    "data = extract_features(METHOD, PROCESS_INFO, DATASET_PATH, DURATION, SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f0529f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b945ce3f3348efb9690b4caed7297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=140)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_FFT = 512\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "# will trim data to this duration in sec\n",
    "DURATION = 16\n",
    "SIGNAL_LEN = SAMPLE_RATE * DURATION\n",
    "TRAIN_INFO = {\"duration\": DURATION, \"n_fft\": N_FFT, \"sample_rate\": SAMPLE_RATE, \"trim\": true, \"zero-pad\": true}\n",
    "\n",
    "\n",
    "# dictionary to store mapping, labels, and spectograms\n",
    "data = {\n",
    "    \"mapping\": [],\n",
    "    \"labels\": [],\n",
    "    \"stft\": [],\n",
    "    \"train_info\": TRAIN_INFO \n",
    "}\n",
    "\n",
    "audio_files = glob(f\"./data/filtered/*/*.wav\")\n",
    "\n",
    "# instantiate the progress bar\n",
    "f = IntProgress(min=0, max=len(audio_files)) \n",
    "display(f) # display the bar\n",
    "    \n",
    "for file in audio_files:\n",
    "    f.value += 1\n",
    "    lang = file.split(\"/\")[3]\n",
    "    data[\"mapping\"].append(lang)\n",
    "    data[\"labels\"].append(lang_mapping[lang])  \n",
    "\n",
    "    signal, sample_rate = librosa.load(file, mono=True)\n",
    "    \n",
    "    # Trim leading and trailing silence from an audio signal\n",
    "    signal_trimmed, _ =  librosa.effects.trim(signal, top_db=20)\n",
    "    \n",
    "    # cut signal to specified duration or pad with 0s\n",
    "    audio_duration = librosa.get_duration(y=signal_trimmed)\n",
    "    if audio_duration > DURATION:\n",
    "        signal_fin = signal_trimmed[:SIGNAL_LEN]\n",
    "    else:\n",
    "        signal_fin = librosa.util.fix_length(signal_trimmed, size=SIGNAL_LEN)\n",
    "\n",
    "    # 3 CWT\n",
    "    scales = np.arange(1, 200, 5)\n",
    "    waveletname = 'morl'\n",
    "    coeff, freq = pywt.cwt(signal_fin, scales, waveletname, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cwt(dataset_size, signal_size, variables_num, dataset):\n",
    "    scales = range(1, signal_size)\n",
    "    waveletname = 'morl'\n",
    "    data_cwt = np.ndarray(shape=(dataset_size, signal_size-1, signal_size-1, variables_num))\n",
    "    \n",
    "    # instantiate the progress bar\n",
    "    max_count = dataset_size\n",
    "    f = IntProgress(min=0, max=max_count) \n",
    "    display(f) # display the bar\n",
    "    \n",
    "    for ii in range(0, dataset_size):\n",
    "        f.value += 1\n",
    "        for jj in range(0, variables_num):\n",
    "            signal = dataset[ii, :, jj]\n",
    "            # coeff shape (19, 20) bc of 19 scales\n",
    "            coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "            \n",
    "            # subtract 1 to get 19x19 coeffecients that would result into images of size 19x19 \n",
    "            # images are scaleograms produced after cwt\n",
    "            coeff_ = coeff[:,:signal_size-1]\n",
    "            # combine data into a single signal image with 6 channels \n",
    "            # each representing 1 variable\n",
    "            data_cwt[ii, :, :, jj] = coeff_\n",
    "    return data_cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6f982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"./data/stft_data_16sec.json\"\n",
    "\n",
    "if os.path.exists(JSON_PATH):\n",
    "    os.remove(JSON_PATH)\n",
    "\n",
    "with open(JSON_PATH, \"w\") as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac1a9e",
   "metadata": {},
   "source": [
    "# Learning from errors:\n",
    "had problem with uneven array size & np.VisibleDeprecationWarning -> trimming audios made 1 audio file less than 16 sec, which resulted into ragged array -> couldn't properly import as np.array <br>\n",
    "**Solution**: pad with 0s using librosa.util.fix_length<br>\n",
    "Problem desc: <br>\n",
    "\"I faced the np.VisibleDeprecationWarning while stacking lists containing audio data from WAV files. This problem occurred because audio files had different lengths. So, the lists that I needed to stack together into one numpy array also had varying lengths. Ignoring or suppressing this warning did not give the desired stacked np array so, I made all the audio files of the same length by using pydub.AudioSegment as mentioned in this answer.\"\n",
    "https://stackoverflow.com/questions/63097829/debugging-numpy-visibledeprecationwarning-ndarray-from-ragged-nested-sequences\n",
    "\n",
    "\n",
    "UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
    "https://stackoverflow.com/questions/56929874/what-is-the-warning-empty-filters-detected-in-mel-frequency-basis-about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156c942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
